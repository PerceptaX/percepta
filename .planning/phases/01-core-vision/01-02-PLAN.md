---
phase: 01-core-vision
plan: 02
type: execute
---

<objective>
Implement Claude Vision driver and camera capture for hardware observation.

Purpose: Build the perception engine - capture webcam frames and send to Claude Vision API for structured signal extraction. This is the core differentiator.

Output: Working VisionDriver that captures frames, sends to Claude Sonnet 4.5, parses LED/Display/Boot signals with 95%+ accuracy.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-vision/DISCOVERY.md
@.planning/phases/01-core-vision/01-01-SUMMARY.md

**Tech stack from Plan 1:**
- Core types: Signal interface, LEDSignal, DisplaySignal, BootTimingSignal, Observation
- SQLite storage driver functional

**New dependencies (from DISCOVERY):**
- `github.com/anthropics/anthropic-sdk-go` v1.22.1 - Claude Vision API
- `github.com/blackjack/webcam` - Linux V4L2 camera capture

**Key constraints:**
- Must achieve 95%+ accuracy on LED/display/boot signals
- Vision API response is unstructured text - parse with regex
- Don't hand-roll NLP/LLM for parsing (regex sufficient per DISCOVERY)
- API key from environment variable ANTHROPIC_API_KEY
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement camera capture driver (Linux V4L2)</name>
  <files>internal/vision/camera.go</files>
  <action>
Install dependency:
```bash
go get github.com/blackjack/webcam@latest
```

Create `internal/vision/camera.go`:

```go
package vision

import (
    "fmt"
    "github.com/blackjack/webcam"
)

type CameraDriver struct {
    devicePath string
    cam        *webcam.Webcam
}

func NewCameraDriver(devicePath string) *CameraDriver {
    return &CameraDriver{devicePath: devicePath}
}

func (c *CameraDriver) Open() error {
    cam, err := webcam.Open(c.devicePath)
    if err != nil {
        return fmt.Errorf("failed to open camera %s: %w", c.devicePath, err)
    }
    c.cam = cam

    // Set format to MJPEG 1280x720 (balance quality vs size)
    format := webcam.PixelFormat(webcam.V4L2_PIX_FMT_MJPEG)
    _, _, _, err = c.cam.SetImageFormat(format, 1280, 720)
    if err != nil {
        c.cam.Close()
        return fmt.Errorf("failed to set image format: %w", err)
    }

    // Start streaming
    err = c.cam.StartStreaming()
    if err != nil {
        c.cam.Close()
        return fmt.Errorf("failed to start streaming: %w", err)
    }

    return nil
}

func (c *CameraDriver) CaptureFrame() ([]byte, error) {
    if c.cam == nil {
        return nil, fmt.Errorf("camera not opened")
    }

    // Wait for frame (5 second timeout)
    err := c.cam.WaitForFrame(5)
    if err != nil {
        return nil, fmt.Errorf("timeout waiting for frame: %w", err)
    }

    frame, err := c.cam.ReadFrame()
    if err != nil {
        return nil, fmt.Errorf("failed to read frame: %w", err)
    }

    // Return JPEG bytes (MJPEG is already JPEG frames)
    return frame, nil
}

func (c *CameraDriver) Close() error {
    if c.cam != nil {
        c.cam.StopStreaming()
        return c.cam.Close()
    }
    return nil
}
```

Use MJPEG format (already JPEG compressed) at 1280x720 for balance of quality and size. Must call WaitForFrame before ReadFrame. Must call Close() or device stays locked (per DISCOVERY pitfall).
  </action>
  <verify>
- `go build ./internal/vision` succeeds
- Verify /dev/video0 exists on system: `ls /dev/video0`
- If available, test: Open camera, capture frame, verify frame is non-empty JPEG bytes
  </verify>
  <done>Camera driver implemented with Open, CaptureFrame, Close. MJPEG 1280x720 format. Proper cleanup to avoid device lock.</done>
</task>

<task type="auto">
  <name>Task 2: Implement Claude Vision driver with signal parsing</name>
  <files>internal/vision/claude.go, internal/vision/parser.go, internal/core/id.go</files>
  <action>
Install Anthropic SDK:
```bash
go get github.com/anthropics/anthropic-sdk-go@v1.22.1
```

Create `internal/core/id.go` for ID generation:
```go
package core

import (
    "crypto/rand"
    "encoding/hex"
)

func GenerateID() string {
    b := make([]byte, 16)
    rand.Read(b)
    return hex.EncodeToString(b)
}
```

Create `internal/vision/claude.go`:

```go
package vision

import (
    "context"
    "encoding/base64"
    "fmt"
    "os"
    "time"

    "github.com/anthropics/anthropic-sdk-go"
    "github.com/anthropics/anthropic-sdk-go/option"
    "github.com/perceptumx/percepta/internal/core"
)

const HardwarePrompt = `Describe this embedded hardware device precisely.

Focus on:
1. LED states (on/off, color, blinking frequency in Hz)
2. Display content (transcribe ALL visible text exactly)
3. Boot sequence indicators if visible

Format your response as:
LEDs:
- [name/color]: [on/off], [color if visible], [frequency in Hz if blinking]

Displays:
- [type]: "[exact text shown]"

Boot: [duration in seconds if measurable]

Be precise with measurements. Estimate blink frequency in Hz.`

type ClaudeVisionDriver struct {
    client *anthropic.Client
    camera *CameraDriver
}

func NewClaudeVisionDriver(cameraPath string) (*ClaudeVisionDriver, error) {
    apiKey := os.Getenv("ANTHROPIC_API_KEY")
    if apiKey == "" {
        return nil, fmt.Errorf("ANTHROPIC_API_KEY not set")
    }

    client := anthropic.NewClient(option.WithAPIKey(apiKey))
    camera := NewCameraDriver(cameraPath)

    return &ClaudeVisionDriver{
        client: client,
        camera: camera,
    }, nil
}

func (d *ClaudeVisionDriver) Observe(deviceID string) (*core.Observation, error) {
    // Open camera if not open
    if d.camera.cam == nil {
        if err := d.camera.Open(); err != nil {
            return nil, err
        }
        defer d.camera.Close()
    }

    // Capture frame
    frame, err := d.camera.CaptureFrame()
    if err != nil {
        return nil, fmt.Errorf("camera capture failed: %w", err)
    }

    // Encode to base64
    base64Frame := base64.StdEncoding.EncodeToString(frame)

    // Call Claude Vision API
    message, err := d.client.Messages.New(context.Background(), anthropic.MessageNewParams{
        MaxTokens: 1024,
        Model:     anthropic.ModelClaudeSonnet4_5_20250929,
        Messages: []anthropic.MessageParam{
            anthropic.NewUserMessage(
                anthropic.NewImageBlockFromBase64("image/jpeg", base64Frame),
                anthropic.NewTextBlock(HardwarePrompt),
            ),
        },
    })

    if err != nil {
        return nil, fmt.Errorf("vision API call failed: %w", err)
    }

    // Extract text response
    responseText := ""
    for _, block := range message.Content {
        if block.Type == "text" {
            responseText += block.Text
        }
    }

    // Parse signals from response
    signals := ParseVisionResponse(responseText)

    return &core.Observation{
        ID:        core.GenerateID(),
        DeviceID:  deviceID,
        Timestamp: time.Now(),
        Signals:   signals,
    }, nil
}
```

Create `internal/vision/parser.go` with regex-based parsing:

```go
package vision

import (
    "regexp"
    "strconv"
    "strings"
    "github.com/perceptumx/percepta/internal/core"
)

func ParseVisionResponse(text string) []core.Signal {
    var signals []core.Signal

    // Parse LED signals
    // Pattern: "STATUS LED: on, blue, 1.5Hz" or "blue LED is ON" or "blinking at 1Hz"
    ledRegex := regexp.MustCompile(`(?i)([a-z0-9_-]+)?\s*(LED|led)(?:[^.]*)(on|off|blinking)(?:[^.]*?)(?:(\d+(?:\.\d+)?)\s*Hz)?`)
    ledMatches := ledRegex.FindAllStringSubmatch(text, -1)

    for _, match := range ledMatches {
        name := match[1]
        if name == "" {
            name = "UNKNOWN"
        }
        stateStr := strings.ToLower(match[3])

        led := core.LEDSignal{
            Name:       name,
            On:         stateStr == "on" || stateStr == "blinking",
            Confidence: 0.85,  // Default confidence
        }

        // Parse blink frequency if present
        if match[4] != "" {
            if freq, err := strconv.ParseFloat(match[4], 64); err == nil {
                led.BlinkHz = freq
            }
        }

        // Try to extract color from surrounding text
        colors := []string{"red", "green", "blue", "yellow", "white", "orange"}
        for _, color := range colors {
            if strings.Contains(strings.ToLower(text), color) {
                // Simplified color mapping
                switch color {
                case "red":
                    led.Color = core.RGB{R: 255, G: 0, B: 0}
                case "green":
                    led.Color = core.RGB{R: 0, G: 255, B: 0}
                case "blue":
                    led.Color = core.RGB{R: 0, G: 0, B: 255}
                }
                break
            }
        }

        signals = append(signals, led)
    }

    // Parse Display signals
    // Pattern: OLED displays "Ready v2.1" or Display: "Hello World"
    displayRegex := regexp.MustCompile(`(?i)(OLED|LCD|Display)(?:[^"]*)"([^"]+)"`)
    displayMatches := displayRegex.FindAllStringSubmatch(text, -1)

    for _, match := range displayMatches {
        displayType := match[1]
        text := match[2]

        signals = append(signals, core.DisplaySignal{
            Name:       displayType,
            Text:       text,
            Confidence: 0.90,
        })
    }

    return signals
}
```

Use regex parsing (not LLM) as per DISCOVERY.md - sufficient for MVP. Hardware prompt is carefully structured to get parseable output. Must use `anthropic.NewImageBlockFromBase64()` with base64.StdEncoding per DISCOVERY pitfalls.
  </action>
  <verify>
- `go build ./internal/vision` succeeds
- If ANTHROPIC_API_KEY set and camera available: Test Observe(), verify returns Observation with parsed signals
- Check signal types include LEDSignal and/or DisplaySignal with confidence scores
  </verify>
  <done>Claude Vision driver implemented with camera capture, Vision API integration, and regex-based signal parsing. Achieves structured signal extraction from hardware images.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `go build ./...` succeeds
- [ ] Camera driver can open /dev/video0, capture JPEG frame
- [ ] Claude Vision driver accepts deviceID, returns Observation
- [ ] Vision response parsed into LEDSignal/DisplaySignal structs
- [ ] Observation includes ID, DeviceID, Timestamp, Signals
- [ ] No hardcoded API keys (uses ANTHROPIC_API_KEY env var)
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Camera driver functional (Linux V4L2)
- Claude Vision integration working with Sonnet 4.5
- Signal parsing extracts LED/Display data from unstructured text
- End-to-end: camera frame → Vision API → structured Observation
  </success_criteria>

<output>
After completion, create `.planning/phases/01-core-vision/01-02-SUMMARY.md`:

# Phase 1 Plan 2: Vision Engine Summary

**Claude Vision driver and camera capture operational for hardware perception**

## Accomplishments

- Camera driver implemented using blackjack/webcam (Linux V4L2)
- MJPEG 1280x720 capture with proper device cleanup
- Claude Vision API integration with Anthropic SDK v1.22.1
- Regex-based signal parser extracting LED states, blink frequency, display text
- End-to-end observation pipeline: webcam → Claude Sonnet 4.5 → structured signals

## Files Created/Modified

- `internal/vision/camera.go` - V4L2 webcam capture driver
- `internal/vision/claude.go` - Claude Vision API integration
- `internal/vision/parser.go` - Regex parser for LED/Display signals
- `internal/core/id.go` - Observation ID generator

## Decisions Made

- **blackjack/webcam for Linux-first**: Simpler than GoCV, sufficient for MVP. Can add cross-platform later.
- **Regex parsing over LLM**: Sufficient for structured LED/Display extraction, lower latency, no extra API cost.
- **MJPEG 1280x720**: Balance of image quality for Vision API vs size/speed.
- **Hardware-specific prompt**: Carefully structured to get parseable LED/Display output from Claude.

## Issues Encountered

(Note any camera access issues, Vision API rate limits, or parsing edge cases discovered during testing)

## Next Phase Readiness

Ready for 01-03: CLI observe command. Can now capture hardware state and convert to structured observations.
</output>
