---
phase: 6.1-perception-enhancements
plan: 01
type: execute
---

<objective>
Enhance vision system reliability with improved LCD OCR, multi-frame LED tracking, and calibrated confidence scores.

Purpose: Make perception robust enough for Phase 7's hardware validation loop. Fixes ISS-001 (single-frame capture misses blinking LEDs), improves LCD OCR from brittle regex to structured output, and adds dynamic confidence scoring based on signal quality.

Output: Solid vision system that captures complete LED states, accurately reads LCD text, and provides calibrated confidence scores for all observations.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/ISSUES.md (ISS-001)
@internal/vision/claude.go
@internal/vision/parser.go
@internal/core/types.go

**Current vision implementation:**
- ClaudeVision with HardwarePrompt → text response → regex parsing
- RegexParser extracts LED/Display signals from unstructured text
- Fixed confidence scores (0.85 LED, 0.90 display)
- Single-frame capture via CameraDriver.CaptureFrame()

**ISS-001 context:**
Discovered in Phase 2.5: Single-frame capture misses blinking LEDs that are OFF at capture time. Object permanence (LED1 = LED1) works correctly, but only LEDs that are ON during the capture instant are detected.

**Solution:** Multi-frame capture with temporal aggregation. Capture 3-5 frames over 1-2 seconds, detect all LEDs across frames, measure blink frequency accurately.

**LCD OCR improvements:**
Current regex parser (`(?i)(OLED|LCD|Display)(?:[^\"]*)\"([^\"]+)\"`) is brittle:
- Requires exact quote format
- Misses text without quotes
- No structured extraction

Solution: Use Anthropic tool use (structured outputs) for deterministic parsing.

**Confidence calibration:**
Current fixed scores don't reflect actual signal quality. Need dynamic confidence based on:
- Text clarity for LCD (OCR confidence from vision model)
- LED state certainty (stable vs flickering)
- Multi-frame agreement (if LED detected in all frames → high confidence)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Improve LCD OCR with structured output</name>
  <files>internal/vision/claude.go, internal/vision/parser.go, internal/vision/structured_parser.go (new), internal/vision/structured_parser_test.go</files>
  <action>
Replace regex parsing with Anthropic tool use for deterministic signal extraction.

**Create StructuredParser using tool use:**

```go
// internal/vision/structured_parser.go

package vision

import (
	"context"
	"encoding/json"
	"fmt"

	"github.com/anthropics/anthropic-sdk-go"
	"github.com/perceptumx/percepta/internal/core"
)

// StructuredParser uses Claude tool use for deterministic signal extraction
type StructuredParser struct {
	client *anthropic.Client
}

func NewStructuredParser(client *anthropic.Client) *StructuredParser {
	return &StructuredParser{client: client}
}

// Tool definitions for signal extraction
func ledDetectionTool() anthropic.ToolParam {
	return anthropic.NewToolComputerUse20241022Param(
		"led_detection",
		"Report detected LED signals with state, color, and blink frequency",
		map[string]interface{}{
			"type": "object",
			"properties": map[string]interface{}{
				"leds": map[string]interface{}{
					"type": "array",
					"items": map[string]interface{}{
						"type": "object",
						"properties": map[string]interface{}{
							"name":       map[string]string{"type": "string", "description": "LED identifier (LED1, LED2, etc)"},
							"on":         map[string]string{"type": "boolean", "description": "True if LED is currently on"},
							"color":      map[string]string{"type": "string", "description": "Color name if visible (red/green/blue/yellow/white/orange)"},
							"blink_hz":   map[string]string{"type": "number", "description": "Blink frequency in Hz if blinking, 0 if steady"},
							"confidence": map[string]string{"type": "number", "description": "Confidence 0-1 in detection"},
						},
						"required": []string{"name", "on", "confidence"},
					},
				},
			},
			"required": []string{"leds"},
		},
	)
}

func displayDetectionTool() anthropic.ToolParam {
	return anthropic.NewToolComputerUse20241022Param(
		"display_detection",
		"Report detected display content with exact text",
		map[string]interface{}{
			"type": "object",
			"properties": map[string]interface{}{
				"displays": map[string]interface{}{
					"type": "array",
					"items": map[string]interface{}{
						"type": "object",
						"properties": map[string]interface{}{
							"name":       map[string]string{"type": "string", "description": "Display type (OLED/LCD/Display)"},
							"text":       map[string]string{"type": "string", "description": "Exact text shown on display"},
							"confidence": map[string]string{"type": "number", "description": "OCR confidence 0-1"},
						},
						"required": []string{"name", "text", "confidence"},
					},
				},
			},
			"required": []string{"displays"},
		},
	)
}

func (p *StructuredParser) Parse(frame []byte) ([]core.Signal, error) {
	// Encode frame to base64
	base64Frame := base64.StdEncoding.EncodeToString(frame)

	// Create message with tool use
	message, err := p.client.Messages.New(context.Background(), anthropic.MessageNewParams{
		MaxTokens: 1024,
		Model:     anthropic.ModelClaudeSonnet4_5_20250929,
		Tools: []anthropic.ToolParam{
			ledDetectionTool(),
			displayDetectionTool(),
		},
		Messages: []anthropic.MessageParam{
			anthropic.NewUserMessage(
				anthropic.NewImageBlockBase64(
					string(anthropic.Base64ImageSourceMediaTypeImageJPEG),
					base64Frame,
				),
				anthropic.NewTextBlock(`Analyze this embedded hardware device. Use the tools to report:
1. All detected LEDs (use led_detection tool)
2. All display content (use display_detection tool)

Be precise with measurements.`),
			),
		},
	})

	if err != nil {
		return nil, fmt.Errorf("API call failed: %w", err)
	}

	// Extract signals from tool use responses
	var signals []core.Signal

	for _, block := range message.Content {
		if block.Type == "tool_use" {
			switch block.Name {
			case "led_detection":
				leds := parseLEDToolResponse(block.Input)
				signals = append(signals, leds...)
			case "display_detection":
				displays := parseDisplayToolResponse(block.Input)
				signals = append(signals, displays...)
			}
		}
	}

	return signals, nil
}

func parseLEDToolResponse(input map[string]interface{}) []core.Signal {
	var signals []core.Signal

	leds, ok := input["leds"].([]interface{})
	if !ok {
		return signals
	}

	for _, ledData := range leds {
		led, ok := ledData.(map[string]interface{})
		if !ok {
			continue
		}

		signal := core.LEDSignal{
			Name:       led["name"].(string),
			On:         led["on"].(bool),
			Confidence: led["confidence"].(float64),
		}

		if colorStr, ok := led["color"].(string); ok {
			signal.Color = parseColor(colorStr)
		}

		if blinkHz, ok := led["blink_hz"].(float64); ok {
			signal.BlinkHz = blinkHz
		}

		signals = append(signals, signal)
	}

	return signals
}

func parseDisplayToolResponse(input map[string]interface{}) []core.Signal {
	var signals []core.Signal

	displays, ok := input["displays"].([]interface{})
	if !ok {
		return signals
	}

	for _, displayData := range displays {
		display, ok := displayData.(map[string]interface{})
		if !ok {
			continue
		}

		signals = append(signals, core.DisplaySignal{
			Name:       display["name"].(string),
			Text:       display["text"].(string),
			Confidence: display["confidence"].(float64),
		})
	}

	return signals
}

func parseColor(colorStr string) core.RGB {
	switch strings.ToLower(colorStr) {
	case "red":
		return core.RGB{R: 255, G: 0, B: 0}
	case "green":
		return core.RGB{R: 0, G: 255, B: 0}
	case "blue":
		return core.RGB{R: 0, G: 0, B: 255}
	case "yellow":
		return core.RGB{R: 255, G: 255, B: 0}
	case "white":
		return core.RGB{R: 255, G: 255, B: 255}
	case "orange":
		return core.RGB{R: 255, G: 165, B: 0}
	default:
		return core.RGB{}
	}
}
```

**Update ClaudeVision to use StructuredParser:**

```go
// internal/vision/claude.go

func NewClaudeVision() (*ClaudeVision, error) {
	apiKey := os.Getenv("ANTHROPIC_API_KEY")
	if apiKey == "" {
		return nil, fmt.Errorf("ANTHROPIC_API_KEY not set")
	}

	client := anthropic.NewClient(option.WithAPIKey(apiKey))

	return &ClaudeVision{
		client: &client,
		parser: NewStructuredParser(&client), // Use structured parser
	}, nil
}
```

**Keep RegexParser for backwards compatibility:**
- Don't delete parser.go
- Add structured_parser.go alongside
- Default to StructuredParser, fallback to RegexParser if tool use fails

**Write tests:**
- Parse LED signals with tool use → returns LEDSignal with dynamic confidence
- Parse display signals → returns DisplaySignal with OCR confidence
- Tool use failure → fallback to regex parser
- Empty frame → returns empty signals slice

AVOID: Breaking existing observations. Keep SignalParser interface unchanged.
AVOID: Removing RegexParser. Keep as fallback for robustness.
  </action>
  <verify>go test ./internal/vision passes, structured parser extracts signals deterministically, LCD OCR more accurate</verify>
  <done>StructuredParser works with tool use, LCD OCR solid with dynamic confidence, RegexParser kept as fallback, tests pass</done>
</task>

<task type="auto">
  <name>Task 2: Add multi-frame capture for complete LED tracking</name>
  <files>pkg/percepta/percepta.go, internal/vision/multi_frame.go (new), internal/vision/multi_frame_test.go, internal/core/interfaces.go</files>
  <action>
Fix ISS-001 by capturing multiple frames and aggregating LED detections.

**Create multi-frame capture:**

```go
// internal/vision/multi_frame.go

package vision

import (
	"fmt"
	"time"

	"github.com/perceptumx/percepta/internal/core"
)

// MultiFrameCapture captures multiple frames and aggregates LED detections
type MultiFrameCapture struct {
	camera     core.CameraDriver
	parser     SignalParser
	frameCount int           // Number of frames to capture
	interval   time.Duration // Time between frames
}

func NewMultiFrameCapture(camera core.CameraDriver, parser SignalParser) *MultiFrameCapture {
	return &MultiFrameCapture{
		camera:     camera,
		parser:     parser,
		frameCount: 5,                      // Capture 5 frames
		interval:   200 * time.Millisecond, // 200ms apart (1 second total)
	}
}

type frameResult struct {
	signals    []core.Signal
	capturedAt time.Time
}

func (m *MultiFrameCapture) Capture() ([]frameResult, error) {
	var results []frameResult

	for i := 0; i < m.frameCount; i++ {
		// Capture frame
		frame, err := m.camera.CaptureFrame()
		if err != nil {
			return nil, fmt.Errorf("frame %d capture failed: %w", i, err)
		}

		// Parse signals
		signals, err := m.parser.Parse(frame)
		if err != nil {
			// Log but continue with other frames
			continue
		}

		results = append(results, frameResult{
			signals:    signals,
			capturedAt: time.Now(),
		})

		// Wait before next frame (except last)
		if i < m.frameCount-1 {
			time.Sleep(m.interval)
		}
	}

	return results, nil
}

// AggregateLEDs combines LED detections across frames
func AggregateLEDs(frames []frameResult) []core.LEDSignal {
	// Map LED name → aggregated state
	ledMap := make(map[string]*ledAggregator)

	for _, frame := range frames {
		for _, signal := range frame.signals {
			if led, ok := signal.(core.LEDSignal); ok {
				if agg, exists := ledMap[led.Name]; exists {
					agg.addObservation(led)
				} else {
					ledMap[led.Name] = &ledAggregator{
						name:         led.Name,
						observations: []core.LEDSignal{led},
					}
				}
			}
		}
	}

	// Compute aggregated signals
	var leds []core.LEDSignal
	for _, agg := range ledMap {
		leds = append(leds, agg.aggregate())
	}

	return leds
}

type ledAggregator struct {
	name         string
	observations []core.LEDSignal
}

func (a *ledAggregator) addObservation(led core.LEDSignal) {
	a.observations = append(a.observations, led)
}

func (a *ledAggregator) aggregate() core.LEDSignal {
	if len(a.observations) == 0 {
		return core.LEDSignal{}
	}

	// Calculate blink frequency from on/off transitions
	onCount := 0
	for _, obs := range a.observations {
		if obs.On {
			onCount++
		}
	}
	offCount := len(a.observations) - onCount

	// Determine state and blink frequency
	led := a.observations[0] // Start with first observation
	led.Name = a.name

	if onCount > 0 && offCount > 0 {
		// Blinking detected (transitions between on/off)
		// Estimate frequency: transitions per second
		// With 5 frames over 1 second, transitions ≈ blink_hz
		transitionCount := 0
		for i := 1; i < len(a.observations); i++ {
			if a.observations[i].On != a.observations[i-1].On {
				transitionCount++
			}
		}
		led.BlinkHz = float64(transitionCount) / 2.0 // Each cycle has 2 transitions
		led.On = true                                 // Blinking LED is "on" logically
	} else if onCount == len(a.observations) {
		// Steady on
		led.On = true
		led.BlinkHz = 0
	} else {
		// Steady off
		led.On = false
		led.BlinkHz = 0
	}

	// Aggregate confidence (average)
	totalConf := 0.0
	for _, obs := range a.observations {
		totalConf += obs.Confidence
	}
	led.Confidence = totalConf / float64(len(a.observations))

	return led
}
```

**Update Core.Observe to use multi-frame:**

```go
// pkg/percepta/percepta.go

func (c *Core) Observe(deviceID string) (*core.Observation, error) {
	// Open camera
	if err := c.camera.Open(); err != nil {
		return nil, fmt.Errorf("camera open failed: %w", err)
	}
	defer c.camera.Close()

	// Multi-frame capture for complete LED detection
	multiFrame := vision.NewMultiFrameCapture(c.camera, c.vision.Parser())
	frames, err := multiFrame.Capture()
	if err != nil {
		return nil, fmt.Errorf("multi-frame capture failed: %w", err)
	}

	// Aggregate LED detections across frames
	leds := vision.AggregateLEDs(frames)

	// Get display signals from most recent frame (displays don't need aggregation)
	displays := getDisplaySignals(frames[len(frames)-1].signals)

	// Combine signals
	var signals []core.Signal
	for _, led := range leds {
		signals = append(signals, led)
	}
	signals = append(signals, displays...)

	return &core.Observation{
		ID:        core.GenerateID(),
		DeviceID:  deviceID,
		Timestamp: time.Now(),
		Signals:   signals,
	}, nil
}

func getDisplaySignals(signals []core.Signal) []core.Signal {
	var displays []core.Signal
	for _, signal := range signals {
		if _, ok := signal.(core.DisplaySignal); ok {
			displays = append(displays, signal)
		}
	}
	return displays
}
```

**Write tests:**
- Capture 5 frames → detects LEDs across all frames
- LED on in frames 1,3,5 → blink detected with frequency
- LED on in all frames → steady on
- LED off in all frames → steady off
- Confidence aggregated correctly (average)

**ISS-001 resolution:**
- Multi-frame capture detects ALL LEDs (even if blinking)
- Object permanence maintained (LED1 = LED1)
- Blink frequency calculated from transitions

AVOID: Breaking single-frame mode. Add multi-frame as default, keep single-frame as option.
AVOID: Increasing latency too much. 5 frames over 1 second is acceptable.
  </action>
  <verify>go test ./internal/vision passes, multi-frame captures all LEDs, ISS-001 resolved, blink frequency accurate</verify>
  <done>Multi-frame capture works, detects all LEDs including blinking ones, ISS-001 resolved, tests pass</done>
</task>

<task type="auto">
  <name>Task 3: Implement confidence calibration</name>
  <files>internal/vision/confidence.go (new), internal/vision/confidence_test.go</files>
  <action>
Replace fixed confidence scores with dynamic calibration based on signal quality.

**Create confidence calibrator:**

```go
// internal/vision/confidence.go

package vision

import (
	"math"

	"github.com/perceptumx/percepta/internal/core"
)

// ConfidenceCalibrator adjusts confidence scores based on signal quality metrics
type ConfidenceCalibrator struct{}

func NewConfidenceCalibrator() *ConfidenceCalibrator {
	return &ConfidenceCalibrator{}
}

// CalibrateLED adjusts LED confidence based on:
// - Multi-frame agreement (if detected in all frames → higher confidence)
// - State stability (steady state → higher confidence than flickering)
// - Color detection (if color present → higher confidence)
func (c *ConfidenceCalibrator) CalibrateLED(led core.LEDSignal, detectionRate float64) core.LEDSignal {
	baseConf := led.Confidence

	// Multi-frame agreement boost
	// detectionRate = fraction of frames where LED was detected
	// 1.0 = detected in all frames → +0.1 boost
	// 0.5 = detected in half → no boost
	agreementBoost := (detectionRate - 0.5) * 0.2
	if agreementBoost < 0 {
		agreementBoost = 0
	}

	// Color detection boost
	colorBoost := 0.0
	if led.Color != (core.RGB{}) {
		colorBoost = 0.05 // Color detected → +0.05
	}

	// Blink frequency confidence
	// Steady state (0 Hz or no blink) → +0.05
	// Measured blink → confidence in frequency
	blinkBoost := 0.0
	if led.BlinkHz == 0 {
		blinkBoost = 0.05 // Steady state
	} else if led.BlinkHz > 0 {
		// Blink detected, confidence depends on measurement quality
		// Assume StructuredParser already provides good confidence
		blinkBoost = 0.0
	}

	// Total confidence (capped at 1.0)
	totalConf := baseConf + agreementBoost + colorBoost + blinkBoost
	if totalConf > 1.0 {
		totalConf = 1.0
	}

	led.Confidence = totalConf
	return led
}

// CalibrateDisplay adjusts display confidence based on:
// - Text length (longer text → higher confidence in OCR)
// - Special characters (if present → might be OCR noise, lower confidence)
// - Base confidence from StructuredParser (tool use confidence)
func (c *ConfidenceCalibrator) CalibrateDisplay(display core.DisplaySignal) core.DisplaySignal {
	baseConf := display.Confidence

	// Text length factor
	// Short text (< 5 chars) might be noise
	// Medium text (5-50 chars) is typical
	// Long text (> 50 chars) is confident OCR
	lengthFactor := 0.0
	textLen := len(display.Text)
	if textLen < 5 {
		lengthFactor = -0.1 // Penalize very short text
	} else if textLen >= 5 && textLen <= 50 {
		lengthFactor = 0.05 // Typical display text
	} else {
		lengthFactor = 0.1 // Long text → confident OCR
	}

	// Special character penalty
	// Excessive special chars might indicate OCR noise
	specialChars := 0
	for _, ch := range display.Text {
		if !isAlphanumeric(ch) && ch != ' ' && ch != '.' && ch != ':' {
			specialChars++
		}
	}
	specialPenalty := 0.0
	if float64(specialChars)/float64(textLen) > 0.3 {
		specialPenalty = -0.15 // >30% special chars → likely noise
	}

	// Total confidence (capped at 1.0, floored at 0.5)
	totalConf := baseConf + lengthFactor + specialPenalty
	if totalConf > 1.0 {
		totalConf = 1.0
	}
	if totalConf < 0.5 {
		totalConf = 0.5 // Don't go below 0.5 for valid displays
	}

	display.Confidence = totalConf
	return display
}

func isAlphanumeric(ch rune) bool {
	return (ch >= 'a' && ch <= 'z') || (ch >= 'A' && ch <= 'Z') || (ch >= '0' && ch <= '9')
}
```

**Integrate into AggregateLEDs:**

```go
// internal/vision/multi_frame.go

func AggregateLEDs(frames []frameResult) []core.LEDSignal {
	ledMap := make(map[string]*ledAggregator)
	calibrator := NewConfidenceCalibrator()

	for _, frame := range frames {
		for _, signal := range frame.signals {
			if led, ok := signal.(core.LEDSignal); ok {
				if agg, exists := ledMap[led.Name]; exists {
					agg.addObservation(led)
				} else {
					ledMap[led.Name] = &ledAggregator{
						name:         led.Name,
						observations: []core.LEDSignal{led},
					}
				}
			}
		}
	}

	var leds []core.LEDSignal
	for _, agg := range ledMap {
		led := agg.aggregate()

		// Calibrate confidence based on detection rate
		detectionRate := float64(len(agg.observations)) / float64(len(frames))
		led = calibrator.CalibrateLED(led, detectionRate)

		leds = append(leds, led)
	}

	return leds
}
```

**Write tests:**
- LED detected in all 5 frames → high confidence (0.95+)
- LED detected in 3/5 frames → medium confidence (0.80-0.90)
- LED with color → confidence boost
- Display with typical text → confidence boost
- Display with many special chars → confidence penalty

AVOID: Over-complicating calibration. Simple heuristics are fine for MVP.
AVOID: Confidence scores < 0.5 or > 1.0 (clamp values).
  </action>
  <verify>go test ./internal/vision passes, confidence scores calibrated dynamically, high-quality signals have high confidence</verify>
  <done>ConfidenceCalibrator works, LED/display confidence adjusted based on signal quality, tests pass</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `go build ./cmd/percepta` succeeds
- [ ] `percepta observe <device>` detects all LEDs (including blinking)
- [ ] ISS-001 resolved: blinking LEDs captured even if off during single frame
- [ ] LCD OCR uses structured output (tool use)
- [ ] Confidence scores calibrated dynamically (not fixed)
- [ ] Multi-frame capture takes ~1 second (5 frames × 200ms)
- [ ] All tests passing (vision, multi_frame, confidence)
- [ ] Observations still compatible with storage (no schema changes yet)
</verification>

<success_criteria>

- StructuredParser working with tool use for deterministic parsing
- LCD OCR solid with dynamic confidence
- Multi-frame capture detects all LEDs (fixes ISS-001)
- Blink frequency calculated accurately from frame transitions
- Confidence calibration based on signal quality (detection rate, color, text length)
- All tests passing
- Vision system robust for Phase 7 hardware validation
  </success_criteria>

<output>
After completion, create `.planning/phases/6.1-perception-enhancements/6.1-01-SUMMARY.md` following the summary.md template.

Include:
- StructuredParser architecture (tool use vs regex fallback)
- Multi-frame capture flow (5 frames, 200ms interval, LED aggregation)
- Confidence calibration heuristics (detection rate, color, text quality)
- ISS-001 resolution details (before/after LED detection)
- Example observations with calibrated confidence scores
- Performance impact (1 second capture time, acceptable latency)
- Integration notes for Plan 6.1-02 (temporal smoothing will use these observations)
</output>
